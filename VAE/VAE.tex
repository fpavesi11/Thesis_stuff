\documentclass[12pt]{article}

\usepackage{../mystyle}

\title{Variational Auto Encoders}

\author{Federico Pavesi}

\date{\today}

\begin{document}

\maketitle
\section{Variational Auto Encoders}

Variational Auto Encoders \cite{kingma2022autoencoding} \cite{pmlr-v32-rezende14} are a specific class of generative models which take advantage of both neural networks and Variational Bayes. In particular, this approach offers a way to learn both a deep latent-variable model and its corresponding inference model using stochastic gradient  descent\cite{VAEIntro}. 

We are in the setting in which we have a dataset \(X\) composed by observations \(\boldx\) belonging to a space \(\mathcal{X}\), and we would like to estimate the distribution of any \(\boldx\), \(p (\boldx)\), using information contained in dataset \(X\). We suppose there exists a hidden data generating process which is random and involves a latent variable \(\boldz \in \mathcal{Z}\), distributed as \(p (\boldz)\). This way we now have two random variables which interaction is captured by their joint distribution \(p (\boldx, \boldz) \). In a slightly different perspective, we can image the process of generating an observation \(\boldx\) starts from a sample from \(p(\boldz)\), \(\boldz\), which is then used to generate \(\boldx\) via the conditional \(p (\boldx | \boldz) \). It is now clear that, if we have accesso to the \(p (\boldx | \boldz) \) and \(p(\boldz)\), we can replicate the data generating process. Throughout the following discussion, we shall refer to \(\boldx\) and it is understood it can interchangeably represent a single observation or a set of observations (including the complete dataset). This allows to reason in terms of batch of data without having to rewrite any equation.

If we employ a Bayesian perspective of this problem, we can say that \(p(\boldz)\) is our prior distribution which in some sense, help govern the data distribution \cite{VI_review}. The likelihood task is represented by the estimation of \(p (\boldx | \boldz) \), while the inference task is to estimate the posterior \(p (\boldz | \boldx) \). 

Let's first put a focus on the inference task: unfortunately, we do not have access directly to \(p (\boldz | \boldx) \) but, following the Variational Bayes perspective, we can choose an approximate posterior \(q (\boldz | \boldx) \), belonging to class \(\mathcal{Q}\), which we hypothize it is close to the true posterior. In classic Variational Bayes, the solution is represented by choosing a class of known distributions (for example Gaussians), parametrized by \(\phi\), and estimate \(\phi\) via Mean Field approximation.


Another fundamental quantity we need for this process again comes from Variational Inference: the Evidence Lower Bound (ELBO). We can derive it using Jensen Inequality. The log likelihood of any datapoint \(\boldx\) is 

\begin{align}
    \label{eqn:log_likelihood}
    \log \mathcal{L}(\boldx) &= \log p(\boldx) = \log \left( \int p(\boldx|\boldz)p(\boldz)d\boldz \right) \nonumber \\
    &= \log \left( \int \frac{q (\boldz)}{q (\boldz)} p(\boldx|\boldz)p(\boldz)d\boldz \right) \nonumber \\
    &\geq -D_{KL}(q(\boldz) || p(\boldz)) + \mathbb{E}_{q(\boldz)}[\log p(\boldx|\boldz)] \\ &= \mathcal{F}(\boldx) \nonumber
\end{align}

where $\mathcal{F}(\boldx)$ is the ELBO, also called Free Energy \cite{pmlr-v32-rezende14} and it is obtained by Jensen inequality.
Because \(D_{KL}\left[ q(\boldz) \parallel p(\boldz|\boldx) \right]\) is always non-negative, we can write

\begin{align}
    \log \mathcal{L}(\boldx) \geq \mathcal{F}(\boldx)
\end{align}

thus the Free Energy lower bounds the log likelihood: whenever it is maximized, we are approximately maximizing the likelihood objective.


Usually, in the context of variational autoencoders, the interest is not on an uncondintional approximate posterior \(q(\boldz)\), but on a conditional one \(q(\boldz|\boldx)\), such that we can `assign' to each observation a latent code. Plugging the conditional in \eqref{eqn:log_likelihood} it is simple to obtain a version of \eqref{eqn:ELBO} with the conditional posterior. To simplify learning process, we can parametrize both the posterior and the prior. If we denote parameters of the posterior as \(\phi\) and the parameters of the prior as \(\theta\), free energy can be rewritten as

\begin{equation}
    \label{eqn:ELBO}
    \mathcal{F}(\boldx) = -D_{KL}\left[ q_{\phi}(\boldz|\boldx) \parallel p(\boldz) \right] + \mathbb{E}_{q_{\phi}(\boldz|\boldx)} \left[ \log p_{\theta}(\boldx|\boldz) \right]
\end{equation}

Maximizing \(\mathcal{F}(\boldx)\), even with parametrization, it's a complex task and rarely there exist tractable closed form solutions. A promising approach is represented by optimization via Stochastic Gradient Descent (SGD) of the negative free energy.


In variational auto encoders, the idea is to parametrize \(q_{\phi}( \boldz| \boldx)\) via a neural network, called the Encoder and, similarly, to parametrize \(p_{\theta}( \boldx| \boldz)\) via another neural network called the Decoder. Notice with this formulation parameters \(\phi, \theta\) are shared among observations \(\boldx \in  X\) \cite{VAEIntro}. In the original formulation in \cite{kingma2022autoencoding}, the prior \(p( \boldz)\) is chosen to be an isotropic standard gaussian \(\mathcal{N}(\textbf{0}, \textbf{I})\). 


Minimizing \(-\mathcal{F}( \boldx)\) using SGD is not as straightforward as it might seem. Unfortunately, the individual datapoint gradient \(\nabla_{\phi, \theta}(-\mathcal{F}(\boldx))\) is, in general, intractable\cite{VAEIntro}. While the gradient w.r.t. Decoder parameters \(\theta\) is relatively simple to estimate

\begin{align}
    \nabla_{\theta}(-\mathcal{F}(\boldx)) &= \nabla_{\theta} \left( D_{KL}\left[ q_{\phi}(\boldz|\boldx) \parallel p(\boldz) \right] - \mathbb{E}_{q_{\phi}(\boldz|\boldx)} \left[ \log p_{\theta}(\boldx|\boldz) \right] \right) \nonumber \\
    &= \nabla_{\theta} \mathbb{E}_{q_{\phi}(\boldz|\boldx)} \left[\log q_\phi (\boldz|\boldx) -\log p(\boldz) - \log p_\theta (\boldx | \boldz)\right] \nonumber \\
    &= \mathbb{E}_{q_{\phi}(\boldz|\boldx)} \left[\log q_\phi (\boldz|\boldx) -\log p(\boldz) - \nabla_{\theta}\log p_\theta (\boldx | \boldz) \right] 
\end{align}

We can switch integration and derivation because the expectation does not depend on \(\theta\), this quantity can be simply estimated via Monte Carlo approximation using a random sample from \(q_\phi (\boldz|\boldx)\). Instead, this passage in general cannot be done for the gradient w.r.t. \(\phi\)

\begin{align}
    \label{eqn:grad_phi_ELBO}
    \nabla_{\phi}(-\mathcal{F}(\boldx)) &= \nabla_{\phi} \left( D_{KL}\left[ q_{\phi}(\boldz|\boldx) \parallel p(\boldz) \right] - \mathbb{E}_{q_{\phi}(\boldz|\boldx)} \left[ \log p_{\theta}(\boldx|\boldz) \right] \right) \nonumber \\
    &= \nabla_{\phi} \mathbb{E}_{q_{\phi}(\boldz|\boldx)} \left[\log q_\phi (\boldz|\boldx) -\log p(\boldz) - \log p_\theta (\boldx | \boldz)\right] \nonumber \\
    &\neq \mathbb{E}_{q_{\phi}(\boldz|\boldx)} \left[\nabla_{\phi} \log q_\phi (\boldz|\boldx) -\log p(\boldz) - \log p_\theta (\boldx | \boldz) \right] 
\end{align}

This limitation can be solved in the case we have continuous latent variable via a change of variable called the \textit{reparametrization trick}.


If latent random variable \(\boldz \sim q_\phi (\boldz|\boldx)\) can be expressed as some differentiable and invertible transformation of another random variable \(\boldeps\), independent from \(\boldx\) or \(\phi\)

\begin{equation}
    \boldz = g(\boldeps, \phi, \boldx)
\end{equation}

It is possible to rewrite expectation in terms of \(\boldeps\) \cite{kingma2022autoencoding}\cite{VAEIntro}

\begin{align}
    \label{eqn:reparametrization}
    \mathbb{E}_{q_{\phi}(\boldz|\boldx)} \left[f(\boldz) \right] 
    &= \mathbb{E}_{p(\boldeps)} \left[f(\boldz) \right]
\end{align}

For any well defined function \(f\) where \(\boldz\) is in its reparametrized form. We are now in the situation to estimate the gradient for \eqref{eqn:grad_phi_ELBO} as

\begin{align}
    \nabla_{\phi}(-\mathcal{F}(\boldx)) &= \nabla_{\phi} \left( D_{KL}\left[ q_{\phi}(\boldz|\boldx) \parallel p(\boldz) \right] - \mathbb{E}_{q_{\phi}(\boldz|\boldx)} \left[ \log p_{\theta}(\boldx|\boldz)p(\boldz) \right] \right) \nonumber \\
    &= \nabla_{\phi} \mathbb{E}_{q_{\phi}(\boldz|\boldx)} \left[\log q_\phi (\boldz|\boldx) -\log p(\boldz) - \log p_\theta (\boldx | \boldz)\right] \nonumber \\
    &= \nabla_{\phi} \mathbb{E}_{p(\boldeps)} \left[\log q_\phi (\boldz|\boldx) -\log p(\boldz) - \log p_\theta (\boldx | \boldz)\right] \nonumber \\
    &= \mathbb{E}_{p(\boldeps)} \left[\nabla_{\phi} \log q_\phi (\boldz|\boldx) -\log p(\boldz) - \log p_\theta (\boldx | \boldz) \right] 
\end{align}


which can be easily estimated via Monte Carlo approximation using a random sample from \(q_\phi (\boldz|\boldx)\) as for \(\theta\). Henceforth, for simplicity, we shall keep the notation \(q_{\phi}(\boldz|\boldx)\), keeping in mind it is subjected to reparametrization (which anyway, does not affect this quantity).


In some cases in which we know the datapoint posterior distribution (for example, in \cite{kingma2022autoencoding} it is a Gaussian) it is possible to evaluate \(D_{KL}\left[ q_{\phi}(\boldz|\boldx) \parallel p(\boldz) \right]\) in closed form. In all other cases, we need to estimate density \(\log q_\phi (\boldz|\boldx)\) subject to reparametrization

\begin{equation}
    \label{eqn:reparametrized_density}
    \log q_\phi (\boldz|\boldx) = \log p(\boldeps) - \log \left| \det \left( \frac{\partial\boldz}{\partial\boldeps}\right) \right|
\end{equation}

Where the right hand side of term is absolute value of logarithm of the determinant of the derivative of \(\boldz=g(\boldeps)\) w.r.t \(\boldeps\). 
For clarity, we can write the objective estimation when we can evaluate the Kullback-Leibler divergence in closed form as

\begin{equation}
    \label{eqn:ELBO_closed_form}
    \tilde{\mathcal{F}}(\boldx) = -D_{KL}\left[ q_{\phi}(\boldz|\boldx) \parallel p(\boldz) \right] + \frac{1}{M}\sum_{m=1}^M \log p_{\theta}(\boldx|\boldz^{(m)})
\end{equation}

While, in the general case, we can write

\begin{equation}
    \label{eqn:ELBO_general}
    \tilde{\mathcal{F}}(\boldx) = \frac{1}{M}\sum_{m=1}^M \log p(\boldz^{(m)}) - \log q_{\phi}(\boldz^{(m)}|\boldx) + \log p_{\theta}(\boldx|\boldz^{(m)}) 
\end{equation}

Where \(M\) is the number of Monte Carlo samples used to estimate the expectation. It is understood that, under posterior reparametrization, \(\boldz^{(m)} = g(\boldeps^{(m)})\).


Equation \eqref{eqn:reparametrized_density} will be especially important when we will define flow transformations. 


At this point, it is simple to optimize \eqref{eqn:ELBO} using SGD. From a practical perspective, it is to notice when we reach the optimal parameter configuration \(\hat{\phi}, \hat{\theta}\) we actually obtained three important goals. First, we are maximizing the likelihood under a latent variable model, using an approximation of the posterior. Second, we can encode observations from their original space \(\mathcal{X}\), to the latent space \(\mathcal{Z}\) which, in most cases, it is less complex and has desirable properties. A good example is \(\mathcal{X}\) as the space of images of handwritten digits and \(\mathcal{Z} = \mathbb{R}^d\) \cite{kingma2022autoencoding}. Last, if \(D_{KL}\left[ q_{\phi}(\boldz|\boldx) \parallel p(\boldz) \right]\) is sufficiently small, it means the posterior is very close to the prior \(p(\boldz)\), such that a we can directly sample from it and the decoder will be able to reconstruct the associated \(\boldx\). This at the end allows to generate any possibly new point belonging to the data distribution. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Challenges of standard approach}
This approach, despite all its advantages, does not come without issues. In particular, two main issues have been evidentiated in literature: optimization issues and blurriness of generative model.

%  OPTIMIZATION ISSUES
Optimization issues have been evidentiated in \cite{VAEIntro}\cite{bowman-etal-2015-large}\cite{Snderby2016LadderVA} and are mainly related to the posterior. What happens is, at the beginning of neural networks training, \(\log p (\boldx|\boldz)\) is relatively weak, making the state \(q(\boldz|\boldx) \approx p(\boldz)\) attractive. This is an undesirable equilibrium because \(\log p (\boldx|\boldz)\), in general remains weak and it is difficult to escape from this area. The alternative proposed by \cite{bowman-etal-2015-large}\cite{Snderby2016LadderVA} is to assign a weight \(\beta\) to \(D_{KL}\left[ q_{\phi}(\boldz|\boldx) \parallel p(\boldz) \right]\) and slowly anneal it from 0 to 1 over training epochs. Free energy becomes
\begin{equation}
    -\mathcal{F}(\boldx) = \beta D_{KL}\left[ q_{\phi}(\boldz|\boldx) \parallel p(\boldz) \right] - \mathbb{E}_{q_{\phi}(\boldz|\boldx)} \left[ \log p_{\theta}(\boldx|\boldz)p(\boldz) \right]
\end{equation}

It is to notice \cite{Higgins2016betaVAELB} use a similar approach, but they find that a proper \(\beta > 1\) allows to formulate the problem as constrained optimization, which balances latent channel capacity and independence constraints with reconstruction accuracy. 


An alternative proposed in \cite{Kingma2016ImprovedVI} is the \textit{free bits} approach, which ensures, on average, a minimum number of bits of information is encoded per latent variable or per group of latent variables \cite{VAEIntro}. This is done by dividing latent dimensions in \(K\) disjoint groups \(\{\boldz_1,\ldots,\boldz_K: \cup_{i=1}^K\boldz_i = \boldz\}\) and each group should encode a minimum of \(\lambda\) information for each \(j=1,2,\ldots, K\)

\begin{equation}
    -\mathcal{F}(\boldx) = \sum_{j=1}^K \max(\lambda, D_{KL}\left[ q_{\phi}(\boldz_i|\boldx) \parallel p(\boldz_i) \right]) - \mathbb{E}_{q_{\phi}(\boldz|\boldx)} \left[ \log p_{\theta}(\boldx|\boldz)p(\boldz) \right]
\end{equation}


%Bluriness
This has still to be written.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Normalizing flows}
A limitation of the classic approach of variational inference in general is the ability to get to a close approximation of the posterior. In particular, it is easily understood that, because most of the time the posterior approximation \(q_\phi(\boldz|\boldx)\) has a simple form (for example Gaussians), the distance from the true posterior is not negligible. The solution of normalizing flows is a powerful approach and it has been deeply explored for sampling and density estimation \cite{NFReview}, \cite{NFDensReview}. Here we shall propose a concise review with a perspective on the implementation in VAE.

Let's start from a general definition. If we define two measurable spaces $(\mathcal{X}, \mathfrak{A})$ and $(\mathcal{Z}, \mathfrak{F})$ and $g: \mathcal{Z} \rightarrow \mathcal{X}$ as a diffeomorphism between them, if moreover $\mu$ is a measure on $\mathcal{Z}$, it is possible to define a measure on $\mathcal{X}$ by 

\begin{align}
    \label{eq:pushforward_measure}
    \nu(A) &= g_{\#}\mu(A) \nonumber \\
    &= \mu(g^{-1}(A))
\end{align}

For all $A \in \mathfrak{A}$ which is called the pushforward measure. As noted by \cite{NFReview} this definition allows to build an interesting connection between normalizing flows and Optimal Transport \cite{villani2008optimal}. Let's now focus on probability measures $\mathcal{P} \in \mathcal{M}_+^{1}$. Suppose $\mathbf{X}$ is a random variable in $(\mathcal{X}, \mathfrak{A})$ (observed data) and $\mathbf{Z}$ random variable in $(\mathcal{Z}, \mathfrak{F})$ (latent variable, usually defined as `simpler'). On the simpler space we can define measures absolutely continuous w.r.t. the Lebesgue measure which admits density, denoted as $q(\boldz)$. Taking advantage of (\ref{eq:pushforward_measure}), we can define a density on the space of observed data, which is the pushforward measure of $q(\boldz)$ through a diffeomorphism $g: \mathcal{Z} \rightarrow \mathcal{X}$, denoted as $p(\boldx)$

\begin{align} 
    p(\boldx) &= q(g^{-1}(\boldx)) \left| \det \frac{\partial g^{-1}}{\partial \boldx} \right| \nonumber \\
    &= q(\boldz) \left| \det \frac{\partial g}{\partial \boldz} \right|^{-1}
\end{align}

Notice because $g$ is a diffeomorphism, $p(\boldx)$ is also absolutely continuous w.r.t. the Lebesgue measure. Moreover, we can easily employ a combination of diffeomorphisms, it is possible to define a flow of densities

\begin{align}
    \boldx^{(i)} &= g_K \circ g_{K-1} \circ \dots \circ g_1(\boldz_0^{(i)}) 
\end{align}

which inverse is 

\begin{align}
    \boldz^{(i)} &= g^{-1}_1 \circ g^{-1}_{2} \circ \dots \circ g^{-1}_K(\boldx^{(i)})
\end{align}

Sometimes we will refer to the direct flow as $G := g_K \circ g_{K-1} \circ \dots \circ g_1(.)$ and to the inverse flow as $G^{-1}$. 

What is interesting is that, because of the change of variable rule, we can compute the density of the transformed distribution following the generative direction 

\begin{align}
    \log p(\boldx) &= \log q(\boldz_0) - \sum_{k=1}^K \log \left| \det \frac{\partial g_k}{\partial \boldz_{k-1}} \right|
\end{align}

as well as in the normalizing direction

\begin{align}
    \log p(\boldx) &= \log q(g^{-1}(\boldx)) + \sum_{k=1}^K \log \left| \det \frac{\partial g_k^{-1}}{\partial \boldx} \right|
\end{align}


Another very interesting property is what is called the law of the unconsciuos statistician (LOTUS), which states that we can compute the expectation of $\boldz_K$ without explicitly knowning its distribution

\begin{align}
    \mathbb{E}_{q(\boldz_K)}[h(\boldz_K)] &= \mathbb{E}_{q(\boldz_0)}[h(g_K \circ g_{K-1} \circ \dots \circ g_1(\boldz_0))]
\end{align}

which does not require computation of the log-determinant of the Jacobian when $h$ does not depend on $q_K$.


The application to variational inference is straightforward, if we resort the formulation of the ELBO in \eqref{eqn:log_likelihood}, we can easily see that any flow $G$ can be used in principle to reparametrize $q(z)$. One can view this approach as a `complex' reparametrization trick. 


While implementing such approach in a VAE, we must acknowledge we are losing some advantages of normalizing flows. Because we are estimating the posterior, which is of course conditioned on $\boldx$, $q(\boldx)$, we loose the ability to unconditionally sample from the posterior in theory, i.e. we cannot sample from $\boldz_0$ and go through the flow until $\boldz_K$. This limitation is mitigated when the Kullback-Leibler divergence is relatively small. In this case, we can sample from the prior knowing most of the mass of the posterior is concentrated around the prior. Notice this consideration is valid for any VAE, the point made here is that reparametrizing a VAE with a normalizing flow `weakens' the flow itself.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application}

In this section we shall explore in depth variatioanl autoencoders we used in applications. Specifically, we adopted four approaches: a classic VAE with diagonal covariance of the posterior, a VAE with full posterior covariance, a VAE with planar flows and a VAE with Inverse Autoregressive Flows. Each model has a isotropic standard Gaussian prior and the decoder architecture is the same for each model.

\subsection{Decoder}

Because we represented the Neural Network architecture \(boldx \in \mathcal{X}\) as a time series of one hot encodings of layer types and feature values, the decoder is structured as follows. Sample from latent dimension \(\boldz\) is first passed through a stack of linear modules and nonlinear activations (Rectified Linear Units specifically) which produce three outputs: a vector \(\boldx_0\), the beginning of the sequence, and two vectors \(\mathbf{h}\), \(\mathbf{c}\) which are used to initialize the hidden and cell states of a Long Short Term Memory (LSTM) module. This LSTM has the task to reconstruct the sequence of one hot encodings of layer types and feature values. For computational efficiency reasons and stability, we decided to use teacher forcing during training. Below, we propose a pseudo code for the autoregressive LSTM. Notice we set \(\mathbf{h}\), \(\mathbf{c}\) to be vectors of arbitrary dimension, possibly different from input dimension (this allows to overcome complexity constraints). To go back to input dimension, an element-wise linear layer is applyed with output dimension equal to \(\boldx\) dimension.

\begin{algorithm}
    \label{alg:autoregressive_LSTM_train}
    \caption{Autoregressive LSTM: training time (teacher forcing)}
    \begin{algorithmic}
        \Input: \(\boldx \in \mathbb{R}^d\), \(\boldx_0 \in \mathbb{R}^d\), \(\mathbf{h} \in \mathbb{R}^h\), \(\mathbf{c} \in \mathbb{R}^h\)
        \Output: \(\hat{\boldx}\)
        \State \(\mathbf{y} \leftarrow \boldx_0 \parallel \boldx[1:n-1]\) \Comment{Concatenate \(\boldx_0\) and \(\boldx\) without last element} 
        \State \(\hat{\boldx} \leftarrow \text{LSTM}(\mathbf{y}, \mathbf{h}, \mathbf{c})\)
        \For{\(i=1\) to \(N\)}
            \State \(\hat{\boldx}[i] \leftarrow \text{Element-wise Linear}(\hat{\boldx}[i])\)
        \EndFor
        \State \textbf{return} \(\hat{\boldx}\)
    \end{algorithmic}
\end{algorithm}

Pseudo code for the autoregressive step is reported in \ref{alg:autoregressive_LSTM_train}. Notice that, in the case of inference, we cannot use teacher forcing, so we have to recursively generate the current input using previous inputs. This is reported in \ref{alg:autoregressive_LSTM_inference}. Notice in both algorithms, after the passage through the element-wise linear, each output \(\hat{\boldx} \in \mathbb{R}^d\) is split into layer type prediction \(\boldx_{[1:d-1]}\) and feature value prediction \(\boldx_{[d]}\) and the former is passed through a Softmax activation, while the latter through a Sigmoid if we normalized features to be in \([0,1]\), otherwise a ReLU to ensure they are positive. Additionally, during inference, the argmax approximation is applied at each iteration such that inputs resembles the original one hot encoding of layer types.

\begin{algorithm}
    \label{alg:autoregressive_LSTM_inference}
    \caption{Autoregressive LSTM: inference time}
    \begin{algorithmic}
        \Input: \(\boldx_0 \in \mathbb{R}^d\), \(\mathbf{h} \in \mathbb{R}^h\), \(\mathbf{c} \in \mathbb{R}^h\)
        \Output: \(\hat{\boldx}\)
        \State \(\hat{\boldx}[0] \leftarrow \boldx_0\) \Comment An additional position `0' is added to prediction, but it will be removed at the end
        \For{\(i=1\) to \(N\)}
            \State \(\hat{\boldx}[i] \leftarrow \text{LSTM}(\hat{\boldx}, \mathbf{h}, \mathbf{c})\) \Comment Only the last element of the output is kept for each iteration
            \State \(\hat{\boldx}[i] \leftarrow \text{Element-wise Linear}(\hat{\boldx}[i])\)
        \EndFor
        \State \textbf{return} \(\hat{\boldx}[1:n]\) \Comment remove the `0' positin element
    \end{algorithmic}
\end{algorithm}


\subsection{Classic VAE}

The first model we wish to implement is a standard VAE with diagonal covariance as in \cite{kingma2022autoencoding}. The encoder outputs for each datapoint mean and variance of a \(k\)-variate gaussian. In this case the reparametrization trick has a very simple form as we can take advantage of properties of gaussian distributions.

\begin{align}
    \boldz = \boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldeps
\end{align}

where \(\boldsymbol{\mu}\) and \(\boldsymbol{\sigma}\) are the mean and standard deviation of the posterior, \(\odot\) is the element-wise product and \(\boldeps \sim \mathcal{N}(\textbf{0}, \textbf{I})\). 


In this setting, it is possible to evaluate the Kullback-Leibler divergence from a standard isotropic Gaussian in closed form

\begin{align}
    D_{KL} \left[ q_{\phi}(\boldz|\boldx) \parallel p(\boldz) \right] &= \frac{1}{2}\left[\boldsymbol{\mu}^T \boldsymbol{\mu} + tr{\Sigma} - h - \log \left| \Sigma \right| \right] \nonumber \\
    &= \frac{1}{2}\left[\boldsymbol{\mu}^T \boldsymbol{\mu} + \sum_{i=1}^h \sigma_{i}^2 - h - \sum_{i=1}^h \log\sigma_{i}^2\right]
\end{align}

Where \(\boldsymbol{\mu}\) is the mean of the posterior, \(\Sigma\) is the covariance matrix and \(h\) is the dimension of the latent space. The passage from the first equation to the second is possible because \(\Sigma\) is a diagonal matrix. 

\subsection{VAE with full posterior covariance}

The case in which we want to allow for correlation between elements of the posterior needs the estimation of a full covariance matrix from the encoder. This task is, in principle, quite complex as \(\Sigma\) must be symmetric, positive semi-definite. A common approach \cite{VAEIntro} is to use Cholesky decomposition of \(\Sigma\) as a lower triangular matrix \(\mathbf{L}\) such that \(\Sigma = \mathbf{L}\mathbf{L}^T\). This way, we can write the reparametrization trick as

\begin{align}
    \boldz = \boldsymbol{\mu} + \mathbf{L}\boldeps
\end{align}

In practice, we let the encoder estimate three quantities: \(\boldsymbol{\mu}\), \(\mathbf{L}^*\) and \(\boldsymbol{\sigma}\). \(\mathbf{L}^*\) is a lower-diagonal matrix with all 0's on the diagonal. \(\mathbf{L}\) is obtained by \(\mathbf{L} = \mathbf{L}^* + \text{diag}(\boldsymbol{\sigma})\). This way, we can ensure \(\Sigma = \mathbf{L}\mathbf{L}^T\) is well definite. Again, it is possible to estimate the Kullback-Leibler divergence from a standard isotropic Gaussian in closed form

\begin{align}
    D_{KL} \left[ q_{\phi}(\boldz|\boldx) \parallel p(\boldz) \right] &= \frac{1}{2}\left[\boldsymbol{\mu}^T \boldsymbol{\mu} + tr{\Sigma} - k - \log \left| \Sigma \right| \right] 
\end{align}


\subsection{VAE with planar flows}

Planar flows were first introduced in \cite{Mohamed2015VariationalIM} and are a simple way to introduce a sequence of invertible transformations to the latent space. The flow is defined as

\begin{align}
    f(\boldz) = \boldz + \mathbf{u}h(\mathbf{w}^T\boldz + b)
\end{align}

where \(\mathbf{u}\), \(\mathbf{w}\) and \(b\) are learnable parameters and \(h\) is a smooth non-linearity. The determinant of the Jacobian of this transformation is

\begin{align}
    \left| \det \frac{\partial f}{\partial \boldz} \right| = \left| 1 + \mathbf{u}^T h'(\mathbf{w}^T\boldz + b)\mathbf{w} \right|
\end{align}


Plugging this into a VAE means that the encoder must estimate \(\boldsymbol{\mu}\) and \(\boldsymbol{\sigma}\) as in standard VAE. Then a sequence of planar stransformations is applyed to the latent variable, such that the final latent variable is 

\begin{align}
    \boldz_K = f_K \circ f_{K-1} \circ \ldots \circ f_1 \circ g(\boldeps)
\end{align}

where \(g(\boldeps)\) is \(\boldz_0\) under reparametrization trick. Here, the Kullback-Leibler divergence is not in closed form and must be estimated via Monte Carlo approximation. In particular, the density of the posterior under planar flow is

\begin{align}
    \log q_K(\boldz_K) = \log q_{\boldz_0}(\boldz_0) - \sum_{k=1}^K \log \left| 1 + \mathbf{u}_k^T h'(\mathbf{w}_k^T\boldz_{k-1} + b_k)\mathbf{w}_k \right|
\end{align}

And \(\log q_{\boldz_0}(\boldz_0)\) is easily estimated as it is an isotropic gaussian distribution with known parameters \(\boldsymbol{\mu}\) and \(\boldsymbol{\sigma}\). 

\subsection{VAE with Inverse Autoregressive Flows (IAF)}

Inverse Autoregressive Flows were first introduced in \cite{Kingma2016ImprovedVI} and are a more complex way to introduce a sequence of invertible transformations to the latent space. The idea of inverse autoregressive flows is to use an autoregressive transformation on the latent variable to define a complex, but with simple determinant transformation. Suppose \(\boldz \in \mathbb{R}^h\) and we can give an order on dimensions \(h\), such that we can see \(\boldz = (\boldz_1,\ldots,\boldz_t,\ldots,\boldz_h)\). The transformation is defined as

\begin{align}
    \boldz_{t+1} = \text{AR-transform}(\boldz_{1:t})
\end{align}

In particular, if we apply a Gaussian autoregressive transformation, we can write

\begin{align}
    \boldz_{t+1} = \boldsymbol{\mu}_t + \boldsymbol{\sigma}_t \odot \boldz_t
\end{align}

Here, \(\boldsymbol{\mu}_t, \boldsymbol{\sigma}_t\) are the outputs of an autoregressive neural network with input \(\boldz_{1:t}\). The determinant of the Jacobian of this transformation is

\begin{equation}
    \left| \det \frac{\partial f}{\partial \boldz} \right| = \prod_{t=1}^h \sigma_t
\end{equation}

For which the density of the posterior under IAF is

\begin{equation}
    \log q_K(\boldz_K) = \log q_{\boldz_0}(\boldz_0) - \sum_{k=1}^K \sum_{t=1}^h \log \sigma_{k,t}
\end{equation}

In our implementation, we follow the suggestion of \cite{Kingma2016ImprovedVI} to use an autoregressive neural network similar to MADE \cite{pmlr-v37-germain15} but with some slight modifications for numerical stability.
Moreover, it is to notice we let the encoder estimate \(\boldsymbol{\mu}\) and \(\boldsymbol{\sigma}\) as in standard VAE, but an additional hidden state \(\mathbf{h}\) is also estimated and plugged into each autoregressive transform. 

\section{Some types of finite flows}

Historically, the first type of flow introduced was the planar flow by \cite{Mohamed2015VariationalIM}, which is a simple way to introduce a sequence of invertible transformations to the latent space. The flow is defined as

\begin{align}
    f(\boldz) = \boldz + \mathbf{u}h(\mathbf{w}^T\boldz + b)
\end{align}

with \(\mathbf{u}\), \(\mathbf{w}\) and \(b\) being learnable parameters and \(h\) is a smooth non-linearity. The determinant of the Jacobian of this transformation is

\begin{align}
    \left| \det \frac{\partial f}{\partial \boldz} \right| = \left| 1 + \mathbf{u}^T h'(\mathbf{w}^T\boldz + b)\mathbf{w} \right|
\end{align}

This transformation is simple and has a simple determinant, but it is not very expressive. 


\bibliographystyle{plain}
\bibliography{ref}

\end{document}