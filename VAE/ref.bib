%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%            ARTICLES                        %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Auto -Encoding Variational Bayes (Kingma, 2013)
@misc{kingma2022autoencoding,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      year={2022},
      eprint={1312.6114},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

%Stochastic Backpropagationa and Approimate inference in deep generative models 
% (Rendeze, Mohamed, 2014)
@InProceedings{pmlr-v32-rezende14,
  title = 	 {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
  author = 	 {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {1278--1286},
  year = 	 {2014},
  editor = 	 {Xing, Eric P. and Jebara, Tony},
  volume = 	 {32},
  number =       {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/rezende14.pdf},
  url = 	 {https://proceedings.mlr.press/v32/rezende14.html},
  abstract = 	 {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning.   Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound.  We develop stochastic backpropagation – rules for gradient backpropagation through stochastic variables – and   derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models.  We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to  generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.}
}

% Bowman (2015) - A large annotated corpus for learning natural language inference
@inproceedings{bowman-etal-2015-large,
    title = "A large annotated corpus for learning natural language inference",
    author = "Bowman, Samuel R.  and
      Angeli, Gabor  and
      Potts, Christopher  and
      Manning, Christopher D.",
    editor = "M{\`a}rquez, Llu{\'\i}s  and
      Callison-Burch, Chris  and
      Su, Jian",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1075",
    doi = "10.18653/v1/D15-1075",
    pages = "632--642",
}

% Sonderby (2016) - Ladder Variational Autoencoders
@inproceedings{Snderby2016LadderVA,
  title={Ladder Variational Autoencoders},
  author={Casper Kaae S{\o}nderby and Tapani Raiko and Lars Maal{\o}e and S{\o}ren Kaae S{\o}nderby and Ole Winther},
  booktitle={Neural Information Processing Systems},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:10447416}
}

% Neural ODE (2018) - Chen
@inproceedings{NEURIPS2018_69386f6b,
 author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Ordinary Differential Equations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf},
 volume = {31},
 year = {2018}
}

%beta-VAE (2016) - Higgins
@inproceedings{Higgins2016betaVAELB,
  title={beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework},
  author={Irina Higgins and Lo{\"i}c Matthey and Arka Pal and Christopher P. Burgess and Xavier Glorot and Matthew M. Botvinick and Shakir Mohamed and Alexander Lerchner},
  booktitle={International Conference on Learning Representations},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:46798026}
}

% IAF (2016) - Kingma
@article{Kingma2016ImprovedVI,
  title={Improved Variational Inference with Inverse Autoregressive Flow},
  author={Diederik P. Kingma and Tim Salimans and Max Welling},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.04934},
  url={https://api.semanticscholar.org/CorpusID:11514441}
}

% Norm flows (2016) - Rezende Mohamed
@article{Mohamed2015VariationalIM,
  title={Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning},
  author={Shakir Mohamed and Danilo Jimenez Rezende},
  journal={ArXiv},
  year={2015},
  volume={abs/1509.08731},
  url={https://api.semanticscholar.org/CorpusID:12860852}
}

% MADE (2015) - Germain
@InProceedings{pmlr-v37-germain15,
  title = 	 {MADE: Masked Autoencoder for Distribution Estimation},
  author = 	 {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {881--889},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/germain15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/germain15.html},
  abstract = 	 {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder’s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.}
}


% Variational Inference: A Review for Statisticians (2017)
@article{VI_review,
author = {David M. Blei, Alp Kucukelbir and Jon D. McAuliffe},
title = {Variational Inference: A Review for Statisticians},
journal = {Journal of the American Statistical Association},
volume = {112},
number = {518},
pages = {859-877},
year = {2017},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2017.1285773},
URL = {https://doi.org/10.1080/01621459.2017.1285773},
eprint = {https://doi.org/10.1080/01621459.2017.1285773}}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%            BOOKS                           %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%An Introduction to Variational Autoencoders (Kingma, Welling, 2019)
@BOOK{VAEIntro,
  author={Kingma, Diederik P. and Welling, Max},
  booktitle={An Introduction to Variational Autoencoders},
  year={2019},
  volume={},
  number={},
  pages={},
  keywords={},
  doi={}}
